\documentclass{article}
\usepackage{amsmath, amsthm, amssymb, amsfonts, bm}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[a4paper]{geometry}
\usepackage{fancyhdr}
\usepackage[algo2e]{algorithm2e}
\fontfamily{cmr}

\title{DD2424 - Project}
%\author{Oskar Stigland \\ stigland@kth.se}

\pagestyle{fancy}
\fancyhf{}
%\rhead{stigland@kth.se}
\lhead{DD2424 - Deep Learning in Data Science}
\rfoot{Page \thepage}

\begin{document}
%\maketitle

	\begin{titlepage}
		\begin{center} 
			
			\rule{\linewidth}{0.5mm}\\[0.5 cm]
			{ \huge \bfseries Sit down, Shakespeare!}\\[0.3 cm]
			{ \bfseries replacing the old bard with recurrent neural networks}\\[0.3cm]
			\rule{\linewidth}{0.5mm}\\[1 cm]
					
			\small\vfill
			\begin{center}
			\centering
			{\large \bfseries \textsc{Summary}}\\
			\vspace{1cm}
			\begin{minipage}{10cm}
				
				...
			\end{minipage}
			\end{center}
			\large\vfill
						

		\end{center}	
		
		\begin{minipage}{0.4\textwidth}
			\begin{flushleft} \small
				%\emph{Student:}\\
				%\textsc{}
				Deep Learning in Data Science\\
				DD2424\\
				Spring 2023
			\end{flushleft}
		\end{minipage}	

	\end{titlepage}

\newpage

\section*{Single-layer LSTM Backpropagation}

\subsection*{Forward pass}
	The forward pass is defined as follows, 
	\begin{align*}
		\bm{i}_t &= \sigma(W_i \bm{h}_{t-1} + U_i\bm{x}_t) \\
		\bm{f}_t &= \sigma(W_f \bm{h}_{t-1} + U_f\bm{x}_t) \\
		\bm{e}_t &= \sigma(W_e \bm{h}_{t-1} + U_e\bm{x}_t) \\
		\tilde{\bm{c}}_t &= \tanh(W_c\bm{h}_{t-1} + U_c\bm{x}_t) \\
		\bm{c}_t &= \bm{f}_t \bullet \bm{c}_{t-1} + \bm{i}_t \bullet \tilde{\bm{c}}_t \\
		\bm{h}_t &= \bm{e}_t \bullet \tanh(\bm{c}_t)\\
		\bm{o}_t &= V \bm{h}_t \\
		\bm{p}_t &= \text{softmax}(\bm{o}_t)
	\end{align*}
	where $\bullet$ denotes element by element multiplication and $\bm{e}_t$ denotes the output/exposure gate. 

\subsection*{Backward pass}
	In order to find the analytical gradients and compute the backward pass, we employ the chain rule. First, we consider the gradient of the cross-entropy loss w.r.t. the output for the final time step. We define 
	$$\frac{\partial L}{\partial \bm{o}_t} = -(\bm{y}_t - \bm{p}_t)^T, \quad \forall\, t = 1, 2, \dots, T$$
	and denote $\bm{g}_t := \frac{\partial L}{\partial \bm{o}_t}$. Then, in order to compute the gradients of the individual weights, we need to first find define the gradients with respect to the hidden activation and the memory cell as these have to be computed through time. Hence, we  first consider the partial derivative of the loss with respect to the hidden units and the memory cell, i.e. $h_t$ and $c_t$ for $t = 1, 2, \dots, T$, and consider two cases: $t= T$ and $t = 1, 2, \dots, T-1$. 

\subsubsection*{I - the case of $t = T$}
	For this case, the gradient computation is straightforward by employing the chain rule. Specifically, for the hidden unit we have that
	$$\frac{\partial L}{\partial \bm{h}_t} = \frac{\partial L}{\partial \bm{o}_t} \frac{\partial\bm{o}_t}{\partial \bm{h}_t} = \bm{g}_t V$$ 
	and for the memory cell we simply get
	$$\frac{\partial L}{\partial \bm{c}_t} = \frac{\partial L}{\partial \bm{o}_t} \frac{\partial\bm{o}_t}{\partial \bm{h}_t} \frac{\partial\bm{h}_t}{\partial \bm{c}_t} = \bm{g}_t V \bm{e}_t (1 - \tanh^2(\bm{c}_t))$$

\subsubsection*{II - the case of $t < T$}
	In order to perform the backward pass for earlier time steps in the sequence, we need to consider how the information propagates forward, i.e. the hidden unit at $t-1$ passes through the hidden unit at $t$. Similarly, the memory cell at $t-1$ passes through the memory cell at $t$. For notational convenience, we define the activations for the gates as $\bm{a}_{., t}$, i.e. such that e.g.  
	$$\bm{i}_t := \sigma(\bm{a}_{i, t})$$ 
	Hence, for the hidden units and the memory cell, we have that
	\begin{align*}
		\frac{\partial L}{\partial \bm{h}_t} &= \frac{\partial L}{\partial \bm{o}_t} \frac{\partial\bm{o}_t}{\partial \bm{h}_t} + \frac{\partial L}{\partial \bm{o}_{t+1}} \frac{\partial\bm{o}_{t+1}}{\partial \bm{h}_t}\\
		&= \bm{g}_t V + \frac{\partial L}{\partial \bm{i}_{t+1}}\frac{\partial \bm{i}_{t+1}}{\partial \bm{h}_t} + \frac{\partial L}{\partial \bm{f}_{t+1}}\frac{\partial \bm{f}_{t+1}}{\partial \bm{h}_t}  + \frac{\partial L}{\partial \bm{e}_{t+1}}\frac{\partial \bm{e}_{t+1}}{\partial \bm{h}_t} + \frac{\partial L}{\partial \tilde{\bm{c}}_{t+1}}\frac{\partial \tilde{\bm{c}}_{t+1}}{\partial \bm{h}_t}\\
		&= \bm{g}_t V  + \frac{\partial L}{\partial \bm{i}_{t+1}} \frac{\partial \bm{i}_{t+1}}{\partial \bm{a}_{i, t+1}} W_i + \frac{\partial L}{\partial \bm{f}_{t+1}}  \frac{\partial \bm{f}_{t+1}}{\partial \bm{a}_{f, t+1}}W_f  + \frac{\partial L}{\partial \bm{e}_{t+1}}  \frac{\partial \bm{e}_{t+1}}{\partial \bm{a}_{e t+1}}W_e + \frac{\partial L}{\partial \tilde{\bm{c}}_{t+1}}  \frac{\partial \tilde{\bm{c}}_{t+1}}{\partial \bm{a}_{c, t+1}}W_c\\
	\\
	\frac{\partial L}{\partial \bm{c}_t} &= \frac{\partial L}{\partial \bm{o}_t} \frac{\partial\bm{o}_t}{\partial \bm{h}_t} \frac{\partial\bm{h}_t}{\partial \bm{c}_t} + \frac{\partial L}{\partial \bm{o}_{t+1}} \frac{\partial\bm{o}_{t+1}}{\partial \bm{h}_{t+1}} \frac{\partial\bm{h}_{t+1}}{\partial \bm{c}_{t+1}} \frac{\partial\bm{c}_{t+1}}{\partial \bm{c}_{t}}\\
	&= \bm{g}_t V \bm{e}_t(1 - \tanh^2(\bm{c}_t)) + \frac{\partial L}{\partial \bm{c}_{t+1}}\frac{\partial\bm{c}_{t+1}}{\partial \bm{c}_{t}} \\
	&= \bm{g}_t V \bm{e}_t(1 - \tanh^2(\bm{c}_t)) + \frac{\partial L}{\partial \bm{c}_{t+1}} \bm{f}_{t+1}
	\end{align*}
	Then, in order to compute the gradients all the gradients for the hidden units and the memory cell, we need to 
	\begin{itemize}
		\item[(1)] calculate gradients for $\bm{i}_{t+1}$, $\bm{f}_{t+1}$, $\bm{e}_{t+1}$, and $\tilde{\bm{c}}_{t+1}$,
		\item[(2)] calculate the gradient for $\bm{h}_{t}$ using $\bm{h}_{t+1}$ and (1), and
		\item[(3)] calculate the gradient for $\bm{c}_{t}$ using (2) and $\bm{c}_{t+1}$.
		\item[(4)] calculate the gradients for $\bm{i}_{t}$, $\bm{f}_{t}$, $\bm{e}_{t}$, and $\tilde{\bm{c}}_{t}$ using (2) and (3).
	\end{itemize}

\subsubsection*{III - gradients for gates and memory cell}
	Further, for notational convenience, we define
	$$ \tilde{\bm{g}}_t := \frac{\partial L}{\partial \bm{h}_t}, \quad\text{and}\quad \hat{\bm{g}}_t := \frac{\partial L}{\partial \bm{c}_t}$$
	such that we can expand the terms in the above equation per the follwing:
	\begin{align*}
		\frac{\partial L}{\partial \bm{i}_{t}}\frac{\partial \bm{i}_{t}}{\partial \bm{h}_{t-1}} &= \hat{\bm{g}}_t\,\tilde{\bm{c}}_t \sigma(\bm{a}_{i, t})(1 - \sigma(\bm{a}_{i, t}))W_i = \hat{\bm{g}}_t\,\tilde{\bm{c}}_t \bm{i}_t (1 - \bm{i}_t) W_i\\
		\frac{\partial L}{\partial \bm{f}_{t}}\frac{\partial \bm{f}_{t}}{\partial \bm{h}_{t-1}} &= \hat{\bm{g}}_t\,\bm{c}_{t-1} \sigma(\bm{a}_{f, t})(1 - \sigma(\bm{a}_{f, t}))W_f = \hat{\bm{g}}_t\,\bm{c}_{t-1} \bm{f}_t (1 - \bm{f}_t) W_f\\ 
		\frac{\partial L}{\partial \bm{e}_{t}}\frac{\partial \bm{e}_{t}}{\partial \bm{h}_{t-1}} &=\tilde{\bm{g}}_t\,  \tanh(\bm{c}_t)\sigma(\bm{a}_{e, t})(1 - \sigma(\bm{a}_{e, t}))W_e = \tilde{\bm{g}}_t\,  \tanh(\bm{c}_t)\bm{e}_t(1 - \bm{e}_t)W_e\\ 
		\frac{\partial L}{\partial \tilde{\bm{c}}_{t}}\frac{\partial \tilde{\bm{c}}_{t}}{\partial \bm{h}_{t-1}} &= \hat{\bm{g}}_t\, \bm{i}_t (1 - \tanh^2(\bm{a}_{c, t}))W_c =  \hat{\bm{g}}_t\, \bm{i}_t (1 - \tilde{\bm{c}}_t^2)W_c
	\end{align*}
%
%\subsubsection*{IV - gradients for activations}
%	In order to properly compute the gradients, we also need to consider the gradients with respect to the activations. Thus, we have that
%	\begin{align*}
%		\frac{\partial \bm{i}_{t}}{\partial \bm{a}_{i, t}} &= \sigma(\bm{a}_{i, t})(1 - \sigma(\bm{a}_{i, t}))\\
%		\frac{\partial \bm{f}_{t}}{\partial \bm{a}_{f, t}} &= \sigma(\bm{a}_{f, t})(1 - \sigma(\bm{a}_{f, t}))\\
%		\frac{\partial \bm{e}_{t}}{\partial \bm{a}_{e, t}} &= \sigma(\bm{a}_{e, t})(1 - \sigma(\bm{a}_{e, t}))\\
%		\frac{\partial \tilde{\bm{c}}_{t}}{\partial \bm{a}_{c, t}} &= 1 - \tanh^2(\bm{a}_{c, t})
%	\end{align*}

\subsubsection*{V - putting it all together}
	Finally, given that we have iteratively computed all the gradients for the hidden units, the gates, the memory cell and their respective activations, we have that
	\begin{align}
		 \frac{\partial L}{\partial W_i} &= \sum_{t=1}^T \hat{\bm{g}}_t\,\tilde{\bm{c}}_t \bm{i}_t (1 - \bm{i}_t) \bm{h}_{t-1} \\
		 \frac{\partial L}{\partial U_i} &= \sum_{t=1}^T \hat{\bm{g}}_t\,\tilde{\bm{c}}_t \bm{i}_t (1 - \bm{i}_t) \bm{x}_{t} \\
		\frac{\partial L}{\partial W_f} &= \sum_{t=1}^T \hat{\bm{g}}_t\,\bm{c}_{t-1} \bm{f}_t (1 - \bm{f}_t) \bm{h}_{t-1} \\ 
		\frac{\partial L}{\partial U_f} &= \sum_{t=1}^T \hat{\bm{g}}_t\,\bm{c}_{t-1} \bm{f}_t (1 - \bm{f}_t) \bm{x}_{t} \\ 
		\frac{\partial L}{\partial W_e} &= \sum_{t=1}^T \tilde{\bm{g}}_t\,  \tanh(\bm{c}_t)\bm{e}_t(1 - \bm{e}_t)\bm{h}_{t-1} \\
		\frac{\partial L}{\partial U_e} &= \sum_{t=1}^T \tilde{\bm{g}}_t\,  \tanh(\bm{c}_t)\bm{e}_t(1 - \bm{e}_t) \bm{x}_{t} \\
		\frac{\partial L}{\partial W_c} &= \sum_{t=1}^T \hat{\bm{g}}_t\, \bm{i}_t (1 - \tilde{\bm{c}}_t^2) \bm{h}_{t-1} \\
		\frac{\partial L}{\partial U_c} &= \sum_{t=1}^T \hat{\bm{g}}_t\, \bm{i}_t (1 - \tilde{\bm{c}}_t^2) \bm{x}_{t} \\
		\frac{\partial L}{\partial V} &= \sum_{t=1}^T \bm{g}_t\bm{h}_t
	\end{align}

\newpage
\section*{Multi-layer LSTM Backpropagation}
\subsection*{Forward pass}
	For a multi-layer LSTM, the forward pass follows that of the single-layer LSTM for each layer, with the exception that for all layers but the first, we replace the original input, $\bm{x}_t$, with the output of the previous layer. That is, for any layer $k$, for $k > 1$, we have that
	$$\bm{x}_t^{(k)} = \bm{o}_t^{(k-1)}$$
	such that we can describe a general forward pass as
	\begin{align*}
		\bm{i}_t^{(k)} &= \sigma(W_i \bm{h}_{t-1}^{(k)} + U_i\bm{o}_t^{(k-1)}) \\
		\bm{f}_t^{(k)} &= \sigma(W_f \bm{h}_{t-1}^{(k)} + U_f\bm{o}_t^{(k-1)}) \\
		\bm{e}_t^{(k)} &= \sigma(W_e \bm{h}_{t-1}^{(k)} + U_e\bm{o}_t^{(k-1)}) \\
		\tilde{\bm{c}}_t^{(k)} &= \tanh(W_c\bm{h}_{t-1}^{(k)} + U_c\bm{o}_t^{(k-1)}) \\
		\bm{c}_t^{(k)} &= \bm{f}_t^{(k)} \bullet \bm{c}_{t-1}^{(k)} + \bm{i}_t^{(k)} \bullet \tilde{\bm{c}}_t^{(k)} \\
		\bm{h}_t^{(k)} &= \bm{e}_t^{(k)} \bullet \tanh(\bm{c}_t^{(k)})\\
		\bm{o}_t^{(k)} &= V \bm{h}_t^{(k)}
	\end{align*}
	where for $k = 1$ we have that $\bm{o}_t^{(k-1)} := \bm{x}_t$ and for $k = K$ we apply a softmax operation to the output, such that the final output of the network is given by
	$$\bm{p}_t = \text{softmax}(\bm{o}_t^{(K)})$$

\subsection*{Backward pass}
	When calculating the gradients for the final layer, i.e. $k = K$, we can proceed as in the case of a single-layer LSTM, with the exception of using $\bm{o}_t^{(K-1)}$ instead of $\bm{x}_t$ from the forward pass. However, when considering the gradients for the preceding layer, i.e. $k = K-1$, we start by considering the gradient of the loss with respect to the output:
	\begin{align*} \frac{\partial L}{\partial \bm{o}_t^{(K-1)}} &= \frac{\partial L}{\partial \bm{i}_t^{(K)}} \frac{\partial \bm{i}_t^{(K)}}{\partial \bm{o}_t^{(K-1)}} + \frac{\partial L}{\partial \bm{f}_t^{(K)}} \frac{\partial \bm{f}_t^{(K)}}{\partial \bm{o}_t^{(K-1)}} + \frac{\partial L}{\partial \bm{e}_t^{(K)}} \frac{\partial \bm{e}_t^{(K)}}{\partial \bm{o}_t^{(K-1)}} + \frac{\partial L}{\partial \tilde{\bm{c}}_t^{(K)}} \frac{\partial \tilde{\bm{c}}_t^{(K)}}{\partial \bm{o}_t^{(K-1)}}\\
	&= \hat{\bm{g}}_t^{(K)}\,\tilde{\bm{c}}_t^{(K)} \bm{i}_t^{(K)} (1 - \bm{i}_t^{(K)}) U_i^{(K)}\\
	&\quad + \hat{\bm{g}}_t^{(K)}\,\bm{c}_{t-1}^{(K)} \bm{f}_t^{(K)} (1 - \bm{f}_t^{(K)}) U_f^{(K)}\\
	&\quad + \tilde{\bm{g}}_t^{(K)}\,  \tanh(\bm{c}_t^{(K)})\bm{e}_t^{(K)}(1 - \bm{e}_t^{(K)})U_e^{(K)}\\
	&\quad +  \hat{\bm{g}}_t^{(K)}\, \bm{i}_t^{(K)} (1 - (\tilde{\bm{c}}_t^{(K)})^2)U_c^{(K)}
	\end{align*}
	We recognize that this iterative pattern is indeed true for all layers for which $k < K$, and define a general expression for the gradient w.r.t. $\bm{o}_t^{(k)}$, and set $\bm{g}_t^{(k)} := \frac{\partial L}{\partial \bm{o}_t^{(k)}}$ such that
	$$\bm{g}_t^{(K)} = -(\bm{y}_t - \bm{p}_t)^T, \quad \forall\, t = 1, 2, \dots, T$$
	and, for $k = 1, 2, \dots, K - 1$:
	\begin{align*}
		\bm{g}_t^{(k)} &= \hat{\bm{g}}_t^{(k+1)}\,\tilde{\bm{c}}_t^{(k+1)} \bm{i}_t^{(k+1)} (1 - \bm{i}_t^{(k+1)}) U_i^{(k+1)} \\
		&\quad + \hat{\bm{g}}_t^{(k+1)}\,\bm{c}_{t-1}^{(k+1)} \bm{f}_t^{(k+1)} (1 - \bm{f}_t^{(k+1)}) U_f^{(k+1)}\\		&\quad + \tilde{\bm{g}}_t^{(k+1)}\,  \tanh(\bm{c}_t^{(k+1)})\bm{e}_t^{(k+1)}(1 - \bm{e}_t^{(k+1)})U_e^{(k+1)}\\
		&\quad +  \hat{\bm{g}}_t^{(k+1)}\, \bm{i}_t^{(k+1)} (1 - (\tilde{\bm{c}}_t^{(k+1)})^2)U_c^{(k+1)}	
	\end{align*}
	for $t = 1, 2, \dots, T$. Now, we may proceed similarly to the one-layer case and define the gradient with respect to the hidden unit and the memory cell:
	$$\tilde{\bm{g}}_t^{(k)} = \frac{\partial L}{\partial \bm{h}_t^{(k)}}\quad\text{and}\quad \hat{\bm{g}}_t^{(k)} = \frac{\partial L}{\partial \bm{c}_t^{(k)}}$$

\subsubsection*{I - the case of $t = T$}
	Given that we have computed the gradients for the next layer, i.e. $k+1$, we can compute $\bm{g}_t^{(k)}$ and then compute the gradient with respect to the hidden unit in layer $k$ as 
	$$\tilde{\bm{g}}_t^{(k)} = \frac{\partial L}{\partial \bm{h}_t^{(k)}} = \bm{g}_t^{(k)}V^{(k)}$$
	and similarly for the memory cell:
	$$\hat{\bm{g}}_t^{(k)} = \frac{\partial L}{\partial \bm{c}_t^{(k)}} = \bm{g}_t^{(k)} V^{(k)} \bm{e}_t^{(k)} (1 - \tanh^2(\bm{c}_t^{(k)}))$$
	for $t = T$. 

\subsubsection*{II - the case of $t < T$}
	Then, for $t = 1, 2, \dots, T-1$, we have for the hidden unit and the memory cell that
	\begin{align*}
	\tilde{\bm{g}}_t^{(k)} &= \bm{g}_t^{(k)} V^{(k)}  + \frac{\partial L}{\partial \bm{i}_{t+1}^{(k)}} \frac{\partial \bm{i}_{t+1}^{(k)}}{\partial \bm{a}_{i, t+1}^{(k)}} W_i^{(k)} + \frac{\partial L}{\partial \bm{f}_{t+1}^{(k)}}  \frac{\partial \bm{f}_{t+1}^{(k)}}{\partial \bm{a}_{f, t+1}^{(k)}}W_f^{(k)}  + \frac{\partial L}{\partial \bm{e}_{t+1}^{(k)}}  \frac{\partial \bm{e}_{t+1}^{(k)}}{\partial \bm{a}_{e t+1}^{(k)}}W_e^{(k)} + \frac{\partial L}{\partial \tilde{\bm{c}}_{t+1}^{(k)}}  \frac{\partial \tilde{\bm{c}}_{t+1}^{(k)}}{\partial \bm{a}_{c, t+1}^{(k)}}W_c^{(k)}\\\\
	\hat{\bm{g}}_t^{(k)} &= \bm{g}_t^{(k)} V^{(k)} \bm{e}_t^{(k)}(1 - \tanh^2(\bm{c}_t^{(k)})) + \frac{\partial L}{\partial \bm{c}_{t+1}^{(k)}} \bm{f}_{t+1}^{(k)}
	\end{align*}

	
	
\end{document}